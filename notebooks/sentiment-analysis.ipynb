{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0            1         2  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                   3  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/labelled_text.csv\", encoding = \"ISO-8859-1\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74682, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74682 entries, 0 to 74681\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       74682 non-null  int64 \n",
      " 1   1       74682 non-null  object\n",
      " 2   2       74682 non-null  object\n",
      " 3   3       73996 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401    6\n",
       "6164    6\n",
       "6141    6\n",
       "6142    6\n",
       "6143    6\n",
       "       ..\n",
       "4678    6\n",
       "4679    6\n",
       "4680    6\n",
       "4681    6\n",
       "9200    6\n",
       "Name: 0, Length: 12447, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TomClancysRainbowSix                 2400\n",
       "MaddenNFL                            2400\n",
       "Microsoft                            2400\n",
       "LeagueOfLegends                      2394\n",
       "CallOfDuty                           2394\n",
       "Verizon                              2382\n",
       "CallOfDutyBlackopsColdWar            2376\n",
       "ApexLegends                          2376\n",
       "Facebook                             2370\n",
       "WorldOfCraft                         2364\n",
       "Dota2                                2364\n",
       "NBA2K                                2352\n",
       "TomClancysGhostRecon                 2346\n",
       "Battlefield                          2346\n",
       "FIFA                                 2340\n",
       "Xbox(Xseries)                        2334\n",
       "Overwatch                            2334\n",
       "johnson&johnson                      2328\n",
       "Amazon                               2316\n",
       "PlayStation5(PS5)                    2310\n",
       "HomeDepot                            2310\n",
       "Cyberpunk2077                        2304\n",
       "CS-GO                                2304\n",
       "GrandTheftAuto(GTA)                  2304\n",
       "Hearthstone                          2298\n",
       "Nvidia                               2298\n",
       "Google                               2298\n",
       "Borderlands                          2286\n",
       "PlayerUnknownsBattlegrounds(PUBG)    2274\n",
       "Fortnite                             2274\n",
       "RedDeadRedemption(RDR)               2262\n",
       "AssassinsCreed                       2244\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      22542\n",
       "Positive      20832\n",
       "Neutral       18318\n",
       "Irrelevant    12990\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available labels\n",
    "df[2].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61692, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[2] != \"Irrelevant\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n",
      "0\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "# Rows where the text is missing\n",
    "print(sum(df[3].isnull()))\n",
    "print(sum(df[3] == \"\"))\n",
    "print(sum(df[3].str.len() == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60831, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=[3], inplace=True)\n",
    "df = df[df[3].str.len() > 1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_id(sentiment):\n",
    "    if sentiment == \"Negative\":\n",
    "        return 0\n",
    "    elif sentiment == \"Neutral\":\n",
    "        return 1\n",
    "    elif sentiment == \"Positive\":\n",
    "        return 2\n",
    "\n",
    "df[2] = df[2].apply(to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: im getting on borderlands and i will murder you all ,\n",
      "   Tokens: ['im', 'getting', 'on', 'border', '##lands', 'and', 'i', 'will', 'murder', 'you', 'all', ',']\n",
      "Token IDs: [13280, 2033, 1113, 3070, 6754, 1105, 178, 1209, 3513, 1128, 1155, 117]\n"
     ]
    }
   ],
   "source": [
    "sample_txt = df.loc[0, 3]\n",
    "\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df[3]:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, messages, labels, tokenizer, max_len):\n",
    "        self.messages = messages\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.messages)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        message = str(self.messages[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            message,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            #pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "          'message': message,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48664, 4), (6083, 4), (6084, 4))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=SEED)\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(\n",
    "    messages=df_train[3].to_numpy(),\n",
    "    labels=df_train[2].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "val_dataset = SentimentDataset(\n",
    "    messages=df_val[3].to_numpy(),\n",
    "    labels=df_val[2].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    messages=df_test[3].to_numpy(),\n",
    "    labels=df_test[2].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=3)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  num_train_epochs=3, \n",
    "                                  per_device_train_batch_size=32)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hasan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 48664\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4563\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: message. If message are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4563' max='4563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4563/4563 30:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.326725</td>\n",
       "      <td>0.879829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.226813</td>\n",
       "      <td>0.926681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.255332</td>\n",
       "      <td>0.938024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer\\checkpoint-500\n",
      "Configuration saved in test_trainer\\checkpoint-500\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer\\checkpoint-1000\n",
      "Configuration saved in test_trainer\\checkpoint-1000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer\\checkpoint-1500\n",
      "Configuration saved in test_trainer\\checkpoint-1500\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6083\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: message. If message are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to test_trainer\\checkpoint-2000\n",
      "Configuration saved in test_trainer\\checkpoint-2000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer\\checkpoint-2500\n",
      "Configuration saved in test_trainer\\checkpoint-2500\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-2500\\pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer\\checkpoint-3000\n",
      "Configuration saved in test_trainer\\checkpoint-3000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6083\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: message. If message are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to test_trainer\\checkpoint-3500\n",
      "Configuration saved in test_trainer\\checkpoint-3500\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-3500\\pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer\\checkpoint-4000\n",
      "Configuration saved in test_trainer\\checkpoint-4000\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer\\checkpoint-4500\n",
      "Configuration saved in test_trainer\\checkpoint-4500\\config.json\n",
      "Model weights saved in test_trainer\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6083\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: message. If message are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to model\n",
      "Configuration saved in model\\config.json\n",
      "Model weights saved in model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./model\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6084\n",
      "  Batch size = 8\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: message. If message are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='761' max='761' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [761/761 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.6445943 , -3.1220152 ,  6.601991  ],\n",
       "       [ 0.16376163, -3.0021856 ,  4.2905874 ],\n",
       "       [-3.9985096 ,  5.3182263 , -1.8554242 ],\n",
       "       ...,\n",
       "       [-2.768107  , -2.0576575 ,  6.814205  ],\n",
       "       [-1.7807797 , -2.156577  ,  6.023279  ],\n",
       "       [-3.8368206 ,  5.4208465 , -2.4999588 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "trained = BertForSequenceClassification.from_pretrained('./model')\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(trained) \n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "raw_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, ..., 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9391847468770546}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=y_pred, references=test_dataset.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./../model\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./../model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./../model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at C:\\Users\\Hasan/.cache\\huggingface\\transformers\\6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Hasan/.cache\\huggingface\\transformers\\ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at C:\\Users\\Hasan/.cache\\huggingface\\transformers\\a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('./../model', id2label={0: 'negative', 1: 'neutral', 2: 'positive'})\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 0.0001364462950732559},\n",
       "  {'label': 'neutral', 'score': 0.000490321428515017},\n",
       "  {'label': 'positive', 'score': 0.9993732571601868}]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"I love this movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 0.999455988407135},\n",
       "  {'label': 'neutral', 'score': 0.0002256113657495007},\n",
       "  {'label': 'positive', 'score': 0.00031838519498705864}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"I hate this game!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'negative', 'score': 0.0013605339918285608},\n",
       "  {'label': 'neutral', 'score': 0.9870943427085876},\n",
       "  {'label': 'positive', 'score': 0.01154518872499466}]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"It is ok\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
